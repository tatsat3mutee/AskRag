# ğŸ” RAG Platform - Production Ready

[![Python](https://img.shields.io/badge/python-3.10%2B-blue)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-Dashboard-FF4B4B)](https://streamlit.io/)
[![FastAPI](https://img.shields.io/badge/FastAPI-API-009688)](https://fastapi.tiangolo.com/)
[![License](https://img.shields.io/badge/license-MIT-green)](docs/LICENSE)

A production-ready **Retrieval-Augmented Generation (RAG)** platform with dual interfaces: Streamlit dashboard and FastAPI REST API. Built with LangChain, HuggingFace embeddings, FAISS vector store, and Groq LLM.

---

## ğŸ¯ Key Features

- ğŸŒ **Dual Interface**: Interactive Streamlit UI + FastAPI REST API
- ğŸ” **Free Embeddings**: HuggingFace Sentence Transformers (no API costs!)
- âš¡ **Fast Search**: FAISS vector database with sub-millisecond queries
- ğŸ¤– **Groq LLM**: Ultra-fast inference for answer generation
- ğŸ“„ **PDF Support**: LangChain-powered document processing
- ğŸ³ **Docker Ready**: Full containerization with Docker Compose
- ğŸ”„ **CI/CD**: GitHub Actions pipeline
- ğŸ“Š **MongoDB**: Optional persistence for document chunks

---

## ğŸ“Š System Architecture

```mermaid
graph TB
    subgraph "User Interfaces"
        A[ğŸ‘¤ User] --> B[ğŸ–¥ï¸ Streamlit Dashboard]
        A --> C[ğŸŒ FastAPI REST API]
    end
    
    subgraph "Application Layer"
        B --> D[app/app.py]
        C --> E[app/main.py]
    end
    
    subgraph "Core Services"
        D --> F[EmbeddingService]
        D --> G[FAISSVectorStore]
        D --> H[Groq LLM]
        E --> F
        E --> G
        E --> H
        
        F --> I[HuggingFace<br/>Sentence Transformers]
        F --> J[LangChain<br/>PDF Loader]
        
        G --> K[(FAISS Index<br/>Vector DB)]
        
        H --> L[Groq API<br/>mixtral-8x7b]
    end
    
    subgraph "Data Storage"
        F -.optional.-> M[(MongoDB<br/>Document Store)]
        G --> N[ğŸ“ faiss_index/<br/>Persistent Store]
    end
    
    subgraph "Configuration"
        O[core/config.py] --> F
        O --> G
        O --> H
    end
    
    style B fill:#FF4B4B
    style C fill:#009688
    style F fill:#FFE082
    style G fill:#81C784
    style H fill:#64B5F6
    style K fill:#FFF59D
    style M fill:#FFF59D
```

---

## ğŸ”„ Data Flow Diagram

```mermaid
sequenceDiagram
    participant User
    participant UI as Streamlit/FastAPI
    participant ES as EmbeddingService
    participant LC as LangChain
    participant HF as HuggingFace
    participant VS as FAISSVectorStore
    participant LLM as Groq LLM
    
    Note over User,LLM: Document Ingestion Flow
    User->>UI: Upload PDF/Text
    UI->>ES: Process document
    ES->>LC: Load & chunk PDF
    LC-->>ES: Text chunks
    ES->>HF: Generate embeddings
    HF-->>ES: Vector embeddings
    ES->>VS: Store vectors + metadata
    VS-->>UI: Success
    
    Note over User,LLM: Query & Retrieval Flow
    User->>UI: Ask question
    UI->>ES: Embed query
    ES->>HF: Generate query embedding
    HF-->>ES: Query vector
    ES->>VS: Search similar vectors
    VS-->>UI: Top-K results
    
    Note over User,LLM: Answer Generation Flow
    UI->>LLM: Question + context
    LLM-->>UI: Generated answer
    UI-->>User: Display results
```

---

## ğŸ“ Project Structure

```
rag-platform/
â”œâ”€â”€ ğŸ“± app/                      # Application layer
â”‚   â”œâ”€â”€ app.py                  # Streamlit dashboard
â”‚   â””â”€â”€ main.py                 # FastAPI REST API
â”‚
â”œâ”€â”€ ğŸ”§ core/                     # Core services
â”‚   â”œâ”€â”€ __init__.py             # Package exports
â”‚   â”œâ”€â”€ config.py               # Configuration management
â”‚   â”œâ”€â”€ embedding_service.py    # HuggingFace + LangChain
â”‚   â”œâ”€â”€ faiss_store.py          # FAISS vector operations
â”‚   â””â”€â”€ opendomain.py           # Groq LLM integration
â”‚
â”œâ”€â”€ ğŸ“š docs/                     # Documentation
â”‚   â”œâ”€â”€ README.md               # This file
â”‚   â”œâ”€â”€ QUICKSTART.md           # 5-minute setup guide
â”‚   â”œâ”€â”€ DEPLOYMENT.md           # Cloud deployment guide
â”‚   â””â”€â”€ LICENSE                 # MIT License
â”‚
â”œâ”€â”€ ğŸ³ Docker/                   # Containerization
â”‚   â”œâ”€â”€ Dockerfile              # Multi-stage build
â”‚   â”œâ”€â”€ docker-compose.yml      # Multi-service orchestration
â”‚   â””â”€â”€ .dockerignore           # Build exclusions
â”‚
â”œâ”€â”€ ğŸ”„ .github/                  # CI/CD
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml              # GitHub Actions pipeline
â”‚
â”œâ”€â”€ ğŸ¨ templates/                # FastAPI HTML templates
â”‚   â””â”€â”€ index.html              # Web UI template
â”‚
â”œâ”€â”€ ğŸ“Š data/                     # Data storage
â”‚   â””â”€â”€ faiss_index/            # FAISS vector index
â”‚
â”œâ”€â”€ âš™ï¸ Configuration
â”‚   â”œâ”€â”€ .env                    # Environment variables (local)
â”‚   â”œâ”€â”€ .env.example            # Template for secrets
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â””â”€â”€ pyproject.toml          # Project metadata
â”‚
â””â”€â”€ ğŸ” Other
    â”œâ”€â”€ .gitignore
    â””â”€â”€ README.md               # Project overview
```

---

## ğŸš€ Quick Start

### 1. **Installation**

```bash
# Clone repository
git clone https://github.com/yourusername/rag-platform.git
cd rag-platform

# Create virtual environment
python -m venv .venv
.venv\Scripts\activate  # Windows
source .venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### 2. **Configuration**

Create `.env` file:

```env
# Required
GROQ_API_KEY=your_groq_api_key_here

# Optional (defaults provided)
HUGGINGFACE_MODEL=sentence-transformers/all-MiniLM-L6-v2
GROQ_MODEL=mixtral-8x7b-32768
VECTOR_STORE=faiss
FAISS_INDEX_PATH=./data/faiss_index
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Optional MongoDB
MONGO_URI=mongodb://localhost:27017
MONGO_DB=rag_db
MONGO_COLLECTION=chunks
```

### 3. **Run Applications**

**Option A: Streamlit Dashboard**
```bash
streamlit run app/app.py
# Opens at http://localhost:8501
```

**Option B: FastAPI REST API**
```bash
uvicorn app.main:app --reload
# API at http://localhost:8000
# Docs at http://localhost:8000/docs
```

**Option C: Docker**
```bash
docker-compose up
# Streamlit: http://localhost:8501
# FastAPI: http://localhost:8000
```

---

## ğŸ”Œ API Endpoints

### **POST /api/embed**
Ingest documents (text or PDF)

```bash
curl -X POST "http://localhost:8000/api/embed" \
  -F "file=@document.pdf" \
  -F "chunk_size=1000" \
  -F "overlap=200" \
  -F "store_mongo=false"
```

### **POST /api/search**
Search and optionally generate answers

```bash
curl -X POST "http://localhost:8000/api/search" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is RAG?",
    "top_k": 5,
    "use_llm": true
  }'
```

---

## ğŸ§© Core Components

### **1. EmbeddingService** (`core/embedding_service.py`)
- Uses HuggingFace Sentence Transformers
- LangChain integration for PDF processing
- Batch embedding generation
- Optional MongoDB persistence

### **2. FAISSVectorStore** (`core/faiss_store.py`)
- Facebook AI Similarity Search
- Persistent index storage
- Fast cosine similarity search
- Metadata management

### **3. Groq Integration** (`core/opendomain.py`)
- Ultra-fast LLM inference
- Context-aware prompting
- Retry logic with exponential backoff
- Mixtral-8x7b model

### **4. Configuration** (`core/config.py`)
- Environment-based config
- Sensible defaults
- Multiple model support
- Easy customization

---

## ğŸ¨ Streamlit Dashboard Features

1. **ğŸ“„ Document Ingestion**
   - Paste text or upload PDFs
   - Configurable chunking
   - Real-time progress
   - Preview chunks

2. **ğŸ” Search & Query**
   - Natural language queries
   - Top-K retrieval
   - Relevance scores
   - Optional LLM answers

3. **âš™ï¸ Configuration**
   - Model selection
   - Parameter tuning
   - API key management
   - System status

---

## ğŸ³ Docker Deployment

```yaml
# docker-compose.yml
services:
  streamlit:
    build: .
    command: streamlit run app/app.py
    ports: ["8501:8501"]
    
  fastapi:
    build: .
    command: uvicorn app.main:app --host 0.0.0.0
    ports: ["8000:8000"]
```

```bash
# Build and run
docker-compose up --build

# Access services
# Streamlit: http://localhost:8501
# FastAPI: http://localhost:8000
```

---

## â˜ï¸ Cloud Deployment

### **Streamlit Cloud** (Recommended for UI)
1. Push to GitHub
2. Visit [share.streamlit.io](https://share.streamlit.io)
3. Connect repository
4. Set main file: `app/app.py`
5. Add secrets in dashboard

### **Railway / Render** (For FastAPI)
```bash
# Procfile
web: uvicorn app.main:app --host 0.0.0.0 --port $PORT
```

See [docs/DEPLOYMENT.md](docs/DEPLOYMENT.md) for detailed guides.

---

## ğŸ§ª Testing

```bash
# Run tests
pytest tests/

# Check imports
python -c "from core import EmbeddingService, FAISSVectorStore; print('âœ“ OK')"

# Test API
curl http://localhost:8000/healthz
```

---

## ğŸ“ˆ Performance

| Metric | Value |
|--------|-------|
| Embedding Speed | ~1000 docs/sec |
| FAISS Search | <1ms for 10K vectors |
| Groq Inference | ~500 tokens/sec |
| Memory Usage | ~500MB base |
| Storage | ~4KB per document chunk |

---

## ğŸ”’ Security

- âœ… Environment-based secrets
- âœ… No hardcoded API keys
- âœ… Input validation
- âœ… Rate limiting support
- âœ… CORS configuration
- âœ… Docker isolation

---

## ğŸ› ï¸ Tech Stack

| Component | Technology | Version |
|-----------|-----------|---------|
| Language | Python | 3.10+ |
| Embeddings | HuggingFace Transformers | Latest |
| PDF Processing | LangChain | Latest |
| Vector DB | FAISS | Latest |
| LLM | Groq API | Mixtral-8x7b |
| Web UI | Streamlit | Latest |
| REST API | FastAPI | Latest |
| Database | MongoDB (optional) | 5.0+ |
| Containers | Docker | Latest |

---

## ğŸ“ Configuration Options

### **Embedding Models**
```python
# Fast & compact (384 dims)
HUGGINGFACE_MODEL="sentence-transformers/all-MiniLM-L6-v2"

# Better quality (768 dims)
HUGGINGFACE_MODEL="sentence-transformers/all-mpnet-base-v2"

# High performance (384 dims)
HUGGINGFACE_MODEL="BAAI/bge-small-en-v1.5"
```

### **Groq Models**
- `mixtral-8x7b-32768` - Best quality (default)
- `llama2-70b-4096` - Fast & efficient
- `gemma-7b-it` - Lightweight

---

## ğŸ¤ Contributing

1. Fork repository
2. Create feature branch: `git checkout -b feature/amazing`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push branch: `git push origin feature/amazing`
5. Open Pull Request

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details

---

## ğŸ†˜ Support

- ğŸ“– [Documentation](docs/)
- ğŸ› [Issues](https://github.com/yourusername/rag-platform/issues)
- ğŸ’¬ [Discussions](https://github.com/yourusername/rag-platform/discussions)

---

## ğŸ™ Acknowledgments

- [LangChain](https://langchain.com/) - Document processing
- [HuggingFace](https://huggingface.co/) - Embeddings
- [FAISS](https://github.com/facebookresearch/faiss) - Vector search
- [Groq](https://groq.com/) - Ultra-fast LLM inference
- [Streamlit](https://streamlit.io/) - Interactive UI
- [FastAPI](https://fastapi.tiangolo.com/) - Modern API framework

---

**Built with â¤ï¸ for the RAG community**
